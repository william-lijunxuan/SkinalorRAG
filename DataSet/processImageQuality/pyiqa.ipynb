{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SkinCAP",
   "id": "8b610bee0c24cb60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "# set file path\n",
    "image_dir = '/home/william/dataset/skin/SkinCAP/skincap'\n",
    "save_dir = '/home/william/dataset/skin/SkinCAP/skincap_HD'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# init model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('brisque', device=device)\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),              \n",
    "    transforms.CenterCrop(224),          \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "score_threshold = 30.0\n",
    "\n",
    "bad_images = []\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "\n",
    "            if score <= score_threshold:\n",
    "                # save High-definition images \n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(f\"Done. {len(bad_images)} low-quality images found and skipped.\")\n"
   ],
   "id": "1153e35084134177",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SKINgpt",
   "id": "62711f31360b16da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "# set file path\n",
    "image_dir = '/home/william/dataset/skin/SKINgpt/image'\n",
    "save_dir = '/home/william/dataset/skin/SKINgpt/image_HD'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# init model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('brisque', device=device)\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),              \n",
    "    transforms.CenterCrop(224),          \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Score threshold: the lower the better (BRISQUE < 30 indicates good quality; the smaller the value, the clearer the image).\n",
    "score_threshold = 30.0\n",
    "\n",
    "bad_images = []\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "\n",
    "            if score <= score_threshold:\n",
    "                # save High-definition images \n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(f\"Done. {len(bad_images)} low-quality images found and skipped.\")\n"
   ],
   "id": "a95ebabb2fc02cd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MMSkinQA",
   "id": "d321f43910cde838"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "# set file path\n",
    "image_dir = '/home/william/dataset/skin/MM-SkinQA'\n",
    "save_dir = '/home/william/dataset/skin/MM-SkinQA/image_HD'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# init model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('brisque', device=device)\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),              \n",
    "    transforms.CenterCrop(224),          \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Score threshold: the lower the better (BRISQUE < 30 indicates good quality; the smaller the value, the clearer the image).\n",
    "score_threshold = 30.0\n",
    "\n",
    "bad_images = []\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "\n",
    "            if score <= score_threshold:\n",
    "                # save High-definition images \n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(f\"Done. {len(bad_images)} low-quality images found and skipped.\")\n"
   ],
   "id": "b934f2df44d76948",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "# Set input and output paths\n",
    "image_dir = '/home/william/dataset/skin/MM-SkinQA'\n",
    "save_dir = '/home/william/dataset/skin/MM-SkinQA/image_HD'\n",
    "save_dir_skipped = '/home/william/dataset/skin/MM-SkinQA/image_HD_skipped'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(save_dir_skipped, exist_ok=True)\n",
    "\n",
    "# Initialize BRISQUE quality metric\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('brisque', device=device)\n",
    "\n",
    "# Image preprocessing: resize and center crop\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Threshold for BRISQUE score (lower is better quality)\n",
    "score_threshold = 30.0\n",
    "\n",
    "bad_images = []\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "\n",
    "            if score <= score_threshold:\n",
    "                # Save high-quality images\n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                # Save low-quality images\n",
    "                image.save(os.path.join(save_dir_skipped, filename))\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(f\"Done. {len(bad_images)} low-quality images saved to {save_dir_skipped}.\")\n"
   ],
   "id": "3f6070b06b0a57ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# skin_disease",
   "id": "437485a7d47c1d66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "# Set input and output paths\n",
    "image_dir = '/home/william/dataset/skin/skin_disease/image'\n",
    "save_dir = '/home/william/dataset/skin/skin_disease/image_HD'\n",
    "save_dir_skipped = '/home/william/dataset/skin/skin_disease/image_HD_skipped'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(save_dir_skipped, exist_ok=True)\n",
    "\n",
    "# Initialize BRISQUE quality metric\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('brisque', device=device)\n",
    "\n",
    "# Image preprocessing: resize and center crop\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Threshold for BRISQUE score (lower is better quality)\n",
    "score_threshold = 30.0\n",
    "\n",
    "bad_images = []\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "\n",
    "            if score <= score_threshold:\n",
    "                # Save high-quality images\n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                # Save low-quality images\n",
    "                image.save(os.path.join(save_dir_skipped, filename))\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(f\"Done. {len(bad_images)} low-quality images saved to {save_dir_skipped}.\")"
   ],
   "id": "5e0c213f3305fc75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# qalign",
   "id": "5f52dc8147dc8a93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "image_dir = '/home/william/dataset/skin/SkinCAP/skincap'\n",
    "save_dir = '/home/william/dataset/skin/SkinCAP/skincap_HD_qalign'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('qalign', device=device)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "])\n",
    "\n",
    "\n",
    "score_threshold = 3.0\n",
    "\n",
    "bad_images = []\n",
    "for filename in tqdm(os.listdir(image_dir), desc=\"Evaluating images (qalign)\"):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = transform(image)\n",
    "\n",
    "\n",
    "            score = iqa_metric([image], task_='quality')[0].item()\n",
    "            print(f\"{filename} score: {score}\") \n",
    "\n",
    "            if score <= score_threshold:\n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(f\"Done. {len(bad_images)} low-quality images found and skipped.\")\n"
   ],
   "id": "539d32b72c0d0fd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# all clipiqa",
   "id": "12732fa801f9dd38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "# set file path\n",
    "image_dir = '/home/william/dataset/skin/SkinCAP/skincap'\n",
    "save_dir = '/home/william/dataset/skin/SkinCAP/skincap_HD_clipiqa'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# init model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('clipiqa', device=device)\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),              \n",
    "    transforms.CenterCrop(224),          \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "score_threshold = 0.5\n",
    "\n",
    "bad_images = []\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "            print(f\"{filename} score: {score}\") \n",
    "            if score <= score_threshold:\n",
    "                # save High-definition images \n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(f\"Done. {len(bad_images)} low-quality images found and skipped.\")\n"
   ],
   "id": "8bbc8007aefaeeb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# train clipiqa",
   "id": "811da86ca52e8f34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "# set file path\n",
    "image_dir = '/home/william/dataset/skin/SkinCAP/SkinCAP_train'\n",
    "save_dir = '/home/william/dataset/skin/SkinCAP/SkinCAP_train_HD_clipiqa'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# init model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('clipiqa', device=device)\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),              \n",
    "    transforms.CenterCrop(224),          \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "score_threshold = 0.5\n",
    "\n",
    "bad_images = []\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "            print(f\"{filename} score: {score}\") \n",
    "            if score <= score_threshold:\n",
    "                # save High-definition images \n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(f\"Done. {len(bad_images)} low-quality images found and skipped.\")\n"
   ],
   "id": "f640ed04d493d641",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# train clipiqa",
   "id": "a09783b6ba335c10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "# set file path\n",
    "image_dir = '/home/william/dataset/skin/SkinCAP/SkinCAP_train'\n",
    "save_dir = '/home/william/dataset/skin/SkinCAP/SkinCAP_train_HD_clipiqa_1000'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# init model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('clipiqa', device=device)\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),              \n",
    "    transforms.CenterCrop(224),          \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "score_threshold = 0.65\n",
    "\n",
    "bad_images = []\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "            print(f\"{filename} score: {score}\") \n",
    "            if score >= score_threshold:\n",
    "                # save High-definition images \n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(f\"Done. {len(bad_images)} low-quality images found and skipped.\")\n"
   ],
   "id": "1251b3ce0c063e99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "\n",
    "image_dir = '/home/william/dataset/skin/SkinCAP/SkinCAP_train'\n",
    "save_dir = '/home/william/dataset/skin/SkinCAP/SkinCAP_train_HD_clipiqa_459'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('clipiqa', device=device)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "score_threshold = 0.7\n",
    "bad_images = []\n",
    "\n",
    "\n",
    "count_0_0_5 = 0\n",
    "count_0_5_0_6 = 0\n",
    "count_0_6_0_65 = 0\n",
    "count_0_65_0_7 = 0\n",
    "count_0_7_plus = 0\n",
    "\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "            print(f\"{filename} score: {score}\")\n",
    "\n",
    "            \n",
    "            if score < 0.5:\n",
    "                count_0_0_5 += 1\n",
    "            elif score < 0.6:\n",
    "                count_0_5_0_6 += 1\n",
    "            elif score < 0.65:\n",
    "                count_0_6_0_65 += 1\n",
    "            elif score < 0.7:\n",
    "                count_0_65_0_7 += 1\n",
    "            else:\n",
    "                count_0_7_plus += 1\n",
    "\n",
    "           \n",
    "            if score >= score_threshold:\n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nScore ranges:\")\n",
    "print(f\"0 - 0.5     : {count_0_0_5} images\")\n",
    "print(f\"0.5 - 0.6  : {count_0_5_0_6} images\")\n",
    "print(f\"0.6 - 0.65  : {count_0_6_0_65} images\")\n",
    "print(f\"0.65 - 0.7  : {count_0_65_0_7} images\")\n",
    "print(f\"> 0.7       : {count_0_7_plus} images\")\n",
    "print(f\"Done. {len(bad_images)} low-quality images found and skipped.\")\n"
   ],
   "id": "92d1afe4bbcbee33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SkinCAP_train_HD_clipiqa_935",
   "id": "596a7ba6d7cb7b69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "\n",
    "image_dir = '/home/william/dataset/skin/SkinCAP/SkinCAP_train'\n",
    "save_dir = '/home/william/dataset/skin/SkinCAP/SkinCAP_train_HD_clipiqa_935'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('clipiqa', device=device)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "score_threshold = 0.61\n",
    "bad_images = []\n",
    "\n",
    "\n",
    "count_0_0_5 = 0\n",
    "count_0_5_0_6 = 0\n",
    "count_0_6_0_65 = 0\n",
    "count_0_65_0_7 = 0\n",
    "count_0_7_plus = 0\n",
    "\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "            print(f\"{filename} score: {score}\")\n",
    "\n",
    "            \n",
    "            if score < 0.5:\n",
    "                count_0_0_5 += 1\n",
    "            elif score < 0.6:\n",
    "                count_0_5_0_6 += 1\n",
    "            elif score < 0.65:\n",
    "                count_0_6_0_65 += 1\n",
    "            elif score < 0.7:\n",
    "                count_0_65_0_7 += 1\n",
    "            else:\n",
    "                count_0_7_plus += 1\n",
    "\n",
    "           \n",
    "            if score >= score_threshold:\n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nScore ranges:\")\n",
    "print(f\"0 - 0.5     : {count_0_0_5} images\")\n",
    "print(f\"0.5 - 0.6  : {count_0_5_0_6} images\")\n",
    "print(f\"0.6 - 0.65  : {count_0_6_0_65} images\")\n",
    "print(f\"0.65 - 0.7  : {count_0_65_0_7} images\")\n",
    "print(f\"> 0.7       : {count_0_7_plus} images\")\n",
    "print(f\"Done. {len(bad_images)} low-quality images found and skipped.\")\n"
   ],
   "id": "d23511d7427167f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SkinCAP_train_HD_clipiqa_1417",
   "id": "8b9bd59e0b92c491"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "\n",
    "image_dir = '/home/william/dataset/skin/SkinCAP/SkinCAP_train'\n",
    "save_dir = '/home/william/dataset/skin/SkinCAP/SkinCAP_train_HD_clipiqa_1417'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('clipiqa', device=device)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "score_threshold = 0.5\n",
    "bad_images = []\n",
    "\n",
    "\n",
    "count_0_0_5 = 0\n",
    "count_0_5_0_6 = 0\n",
    "count_0_6_0_65 = 0\n",
    "count_0_65_0_7 = 0\n",
    "count_0_7_plus = 0\n",
    "\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "            print(f\"{filename} score: {score}\")\n",
    "\n",
    "            \n",
    "            if score < 0.5:\n",
    "                count_0_0_5 += 1\n",
    "            elif score < 0.6:\n",
    "                count_0_5_0_6 += 1\n",
    "            elif score < 0.65:\n",
    "                count_0_6_0_65 += 1\n",
    "            elif score < 0.7:\n",
    "                count_0_65_0_7 += 1\n",
    "            else:\n",
    "                count_0_7_plus += 1\n",
    "\n",
    "           \n",
    "            if score >= score_threshold:\n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nScore ranges:\")\n",
    "print(f\"0 - 0.5     : {count_0_0_5} images\")\n",
    "print(f\"0.5 - 0.6  : {count_0_5_0_6} images\")\n",
    "print(f\"0.6 - 0.65  : {count_0_6_0_65} images\")\n",
    "print(f\"0.65 - 0.7  : {count_0_65_0_7} images\")\n",
    "print(f\"> 0.7       : {count_0_7_plus} images\")\n",
    "print(f\"Done. {len(bad_images)} low-quality images found and skipped.\")\n"
   ],
   "id": "739682807e16c776",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# skincap_HD_clipiqa",
   "id": "cebe32868b3c7940"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "# set file path\n",
    "image_dir = '/home/william/dataset/skin/SkinCAP/skincap'\n",
    "save_dir = '/home/william/dataset/skin/SkinCAP/skincap_HD_clipiqa_2936'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# init model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('clipiqa', device=device)\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),              \n",
    "    transforms.CenterCrop(224),          \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "score_threshold = 0.5\n",
    "\n",
    "bad_images = []\n",
    "\n",
    "count_0_0_5 = 0\n",
    "count_0_5_0_6 = 0\n",
    "count_0_6_0_65 = 0\n",
    "count_0_65_0_7 = 0\n",
    "count_0_7_plus = 0\n",
    "\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "            print(f\"{filename} score: {score}\") \n",
    "            if score < 0.5:\n",
    "                count_0_0_5 += 1\n",
    "            elif score < 0.6:\n",
    "                count_0_5_0_6 += 1\n",
    "            elif score < 0.65:\n",
    "                count_0_6_0_65 += 1\n",
    "            elif score < 0.7:\n",
    "                count_0_65_0_7 += 1\n",
    "            else:\n",
    "                count_0_7_plus += 1\n",
    "            \n",
    "            if score >= score_threshold:\n",
    "                # save High-definition images \n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(f\"\\nScore ranges:\")\n",
    "print(f\"0 - 0.5     : {count_0_0_5} images\")\n",
    "print(f\"0.5 - 0.6  : {count_0_5_0_6} images\")\n",
    "print(f\"0.6 - 0.65  : {count_0_6_0_65} images\")\n",
    "print(f\"0.65 - 0.7  : {count_0_65_0_7} images\")\n",
    "print(f\"> 0.7       : {count_0_7_plus} images\")\n",
    "print(f\"Done. {len(bad_images)} low-quality images found and skipped.\")\n"
   ],
   "id": "5ed8e2da13cc2392",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "# set file path\n",
    "image_dir = '/home/william/dataset/skin/SkinCAP/skincap'\n",
    "save_dir = '/home/william/dataset/skin/SkinCAP/skincap_HD_clipiqa_955'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# init model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('clipiqa', device=device)\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),              \n",
    "    transforms.CenterCrop(224),          \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "score_threshold = 0.7\n",
    "\n",
    "bad_images = []\n",
    "\n",
    "count_0_0_5 = 0\n",
    "count_0_5_0_6 = 0\n",
    "count_0_6_0_65 = 0\n",
    "count_0_65_0_7 = 0\n",
    "count_0_7_plus = 0\n",
    "\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "            print(f\"{filename} score: {score}\") \n",
    "            if score < 0.5:\n",
    "                count_0_0_5 += 1\n",
    "            elif score < 0.6:\n",
    "                count_0_5_0_6 += 1\n",
    "            elif score < 0.65:\n",
    "                count_0_6_0_65 += 1\n",
    "            elif score < 0.7:\n",
    "                count_0_65_0_7 += 1\n",
    "            else:\n",
    "                count_0_7_plus += 1\n",
    "            \n",
    "            if score >= score_threshold:\n",
    "                # save High-definition images \n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(f\"\\nScore ranges:\")\n",
    "print(f\"0 - 0.5     : {count_0_0_5} images\")\n",
    "print(f\"0.5 - 0.6  : {count_0_5_0_6} images\")\n",
    "print(f\"0.6 - 0.65  : {count_0_6_0_65} images\")\n",
    "print(f\"0.65 - 0.7  : {count_0_65_0_7} images\")\n",
    "print(f\"> 0.7       : {count_0_7_plus} images\")\n",
    "print(f\"Done. {len(bad_images)} low-quality images found and skipped.\")\n"
   ],
   "id": "da23abf6592a5428",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pyiqa\n",
    "\n",
    "# set file path\n",
    "image_dir = '/home/william/dataset/skin/SkinCAP/skincap'\n",
    "save_dir = '/home/william/dataset/skin/SkinCAP/skincap_HD_clipiqa_2042'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# init model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqa_metric = pyiqa.create_metric('clipiqa', device=device)\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),              \n",
    "    transforms.CenterCrop(224),          \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "score_threshold = 0.6\n",
    "\n",
    "bad_images = []\n",
    "\n",
    "count_0_0_5 = 0\n",
    "count_0_5_0_6 = 0\n",
    "count_0_6_0_65 = 0\n",
    "count_0_65_0_7 = 0\n",
    "count_0_7_plus = 0\n",
    "\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            score = iqa_metric(image_tensor).item()\n",
    "            print(f\"{filename} score: {score}\") \n",
    "            if score < 0.5:\n",
    "                count_0_0_5 += 1\n",
    "            elif score < 0.6:\n",
    "                count_0_5_0_6 += 1\n",
    "            elif score < 0.65:\n",
    "                count_0_6_0_65 += 1\n",
    "            elif score < 0.7:\n",
    "                count_0_65_0_7 += 1\n",
    "            else:\n",
    "                count_0_7_plus += 1\n",
    "            \n",
    "            if score >= score_threshold:\n",
    "                # save High-definition images \n",
    "                image.save(os.path.join(save_dir, filename))\n",
    "            else:\n",
    "                bad_images.append((filename, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(f\"\\nScore ranges:\")\n",
    "print(f\"0 - 0.5     : {count_0_0_5} images\")\n",
    "print(f\"0.5 - 0.6  : {count_0_5_0_6} images\")\n",
    "print(f\"0.6 - 0.65  : {count_0_6_0_65} images\")\n",
    "print(f\"0.65 - 0.7  : {count_0_65_0_7} images\")\n",
    "print(f\"> 0.7       : {count_0_7_plus} images\")\n",
    "print(f\"Done. {len(bad_images)} low-quality images found and skipped.\")\n"
   ],
   "id": "c3de028b5109f8e7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
